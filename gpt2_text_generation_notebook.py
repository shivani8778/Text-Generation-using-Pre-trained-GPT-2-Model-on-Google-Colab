# -*- coding: utf-8 -*-
"""GPT2_Text_Generation_Notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s98H7EKxYPY8b34SsYNhNy-cKytpvXen

# Text Generation using GPT-2
This notebook demonstrates how to use the pre-trained GPT-2 model from Hugging Face Transformers to generate coherent text paragraphs based on user prompts.
"""

# Install required libraries (uncomment if running for the first time)
# !pip install transformers
# !pip install torch

from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

# Load pre-trained GPT-2 model and tokenizer
model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

# Define text generation function
def generate_text(prompt, max_length=150, temperature=0.7):
    input_ids = tokenizer.encode(prompt, return_tensors="pt")
    output = model.generate(
        input_ids,
        max_length=max_length,
        num_return_sequences=1,
        no_repeat_ngram_size=2,
        temperature=temperature,
        top_k=50,
        top_p=0.95,
        do_sample=True,
        early_stopping=True
    )
    return tokenizer.decode(output[0], skip_special_tokens=True)

# Example prompts
prompts = [
    "The future of artificial intelligence is",
    "Climate change and its global impact",
    "The benefits of exercise on mental health"
]

# Generate and print text for each prompt
for prompt in prompts:
    print(f"""
Prompt: {prompt}""")
    print("Generated Paragraph:")
    print(generate_text(prompt))

